# é¡¹ç›®ç»“æ„è¯´æ˜

## ğŸ“ å®Œæ•´ç›®å½•æ ‘

```
alpha_transformer_system/
â”‚
â”œâ”€â”€ ğŸ“‹ README.md                    # é¡¹ç›®æ¦‚è§ˆ
â”œâ”€â”€ ğŸ“– QUICKSTART.md                # å¿«é€Ÿå¼€å§‹æŒ‡å—
â”œâ”€â”€ ğŸ“ DESIGN.md                    # ç³»ç»Ÿè®¾è®¡æ–‡æ¡£
â”œâ”€â”€ ğŸ“ PROJECT_STRUCTURE.md         # æœ¬æ–‡ä»¶
â”œâ”€â”€ ğŸ”§ config.py                    # å…¨å±€é…ç½®
â”œâ”€â”€ ğŸš€ main.py                      # ä¸»å…¥å£
â”œâ”€â”€ ğŸ“¦ requirements.txt             # ä¾èµ–åˆ—è¡¨
â”œâ”€â”€ ğŸš« .gitignore                   # Gitå¿½ç•¥æ–‡ä»¶
â”‚
â”œâ”€â”€ ğŸ“Š data/                        # æ•°æ®å±‚
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ collector.py                # å†å²æ•°æ®é‡‡é›†
â”‚   â”œâ”€â”€ preprocessor.py             # æ•°æ®é¢„å¤„ç†
â”‚   â”œâ”€â”€ raw/                        # åŸå§‹æ•°æ®å­˜å‚¨
â”‚   â”‚   â””â”€â”€ alphas_YYYYMMDD_HHMMSS.csv
â”‚   â””â”€â”€ processed/                  # é¢„å¤„ç†åæ•°æ®
â”‚       â”œâ”€â”€ dataset.pkl
â”‚       â”œâ”€â”€ tokenizer.pkl
â”‚       â””â”€â”€ scaler.pkl
â”‚
â”œâ”€â”€ ğŸ§  models/                      # æ¨¡å‹å±‚
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ tokenizer.py                # è¡¨è¾¾å¼åˆ†è¯å™¨
â”‚   â”œâ”€â”€ alpha_transformer.py        # Transformeræ¨¡å‹
â”‚   â””â”€â”€ trainer.py                  # è®­ç»ƒå™¨
â”‚
â”œâ”€â”€ ğŸ­ factories/                   # å·¥å‚å±‚
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ smart_factory.py            # AIå¢å¼ºå·¥å‚
â”‚
â”œâ”€â”€ ğŸ¨ ui/                          # UIå±‚
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ app.py                      # Gradio Webç•Œé¢
â”‚
â”œâ”€â”€ ğŸ”§ utils/                       # å·¥å…·å±‚
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ wq_client.py                # WorldQuant APIå®¢æˆ·ç«¯
â”‚
â”œâ”€â”€ ğŸ’¾ checkpoints/                 # æ¨¡å‹æ£€æŸ¥ç‚¹
â”‚   â”œâ”€â”€ best_model.pt               # æœ€ä½³æ¨¡å‹
â”‚   â””â”€â”€ checkpoint_epoch_*.pt       # å®šæœŸæ£€æŸ¥ç‚¹
â”‚
â”œâ”€â”€ ğŸ“Š logs/                        # è®­ç»ƒæ—¥å¿—
â”‚   â””â”€â”€ training_*.log
â”‚
â””â”€â”€ ğŸ“š examples/                    # ç¤ºä¾‹ä»£ç 
    â””â”€â”€ demo_workflow.py            # å®Œæ•´å·¥ä½œæµç¤ºä¾‹
```

---

## ğŸ“„ æ ¸å¿ƒæ–‡ä»¶è¯¦è§£

### é…ç½®æ–‡ä»¶

#### `config.py`
å…¨å±€é…ç½®ç®¡ç†ï¼ŒåŒ…å«ï¼š
- **WorldQuantConfig**: APIè®¤è¯ã€å›æµ‹å‚æ•°
- **TransformerConfig**: æ¨¡å‹æ¶æ„ã€è®­ç»ƒè¶…å‚æ•°
- **FactoryConfig**: Alphaå·¥å‚å‚æ•°
- **UIConfig**: ç•Œé¢é…ç½®

**ä¿®æ”¹å»ºè®®**ï¼š
```python
# è°ƒæ•´æ¨¡å‹å¤§å°ï¼ˆå†…å­˜ä¸è¶³æ—¶ï¼‰
config.transformer.d_model = 128
config.transformer.batch_size = 16

# è°ƒæ•´æ•°æ®é›†
config.factory.dataset_id = "fundamental6"

# è°ƒæ•´è®­ç»ƒè½®æ•°
config.transformer.num_epochs = 50
```

#### `requirements.txt`
Pythonä¾èµ–åŒ…åˆ—è¡¨ï¼Œå…³é”®ä¾èµ–ï¼š
- `torch`: æ·±åº¦å­¦ä¹ æ¡†æ¶
- `transformers`: Transformeråº“
- `gradio`: Web UIæ¡†æ¶
- `pandas`, `numpy`: æ•°æ®å¤„ç†
- `requests`: APIè°ƒç”¨

---

### ä¸»å…¥å£

#### `main.py`
å‘½ä»¤è¡Œæ¥å£ï¼Œæ”¯æŒå¤šç§è¿è¡Œæ¨¡å¼ï¼š

```bash
# å¯åŠ¨UI
python main.py ui

# æ•°æ®é‡‡é›†
python main.py collect --start-date 01-01 --end-date 12-31

# é¢„å¤„ç†
python main.py preprocess --target-metric combined

# è®­ç»ƒæ¨¡å‹
python main.py train --epochs 50

# ç”ŸæˆAlpha
python main.py generate --generation-size 10000 --top-k 1000
```

---

## ğŸ“¦ æ¨¡å—è¯´æ˜

### Data Layer (`data/`)

#### `collector.py`
**åŠŸèƒ½**ï¼šä»WorldQuant Brainé‡‡é›†å†å²Alphaæ•°æ®

**å…³é”®ç±»**ï¼š`AlphaDataCollector`

**ä¸»è¦æ–¹æ³•**ï¼š
- `collect_historical_alphas()`: é‡‡é›†æ•°æ®
- `load_existing_data()`: åŠ è½½å·²æœ‰æ•°æ®
- `get_statistics()`: æ˜¾ç¤ºæ•°æ®ç»Ÿè®¡

**è¾“å‡º**ï¼š
- CSVæ–‡ä»¶ï¼ˆ`data/raw/alphas_*.csv`ï¼‰
- åŒ…å«expression, sharpe, fitnessç­‰å­—æ®µ

#### `preprocessor.py`
**åŠŸèƒ½**ï¼šæ•°æ®æ¸…æ´—ã€ç‰¹å¾å·¥ç¨‹ã€æ•°æ®åˆ†å‰²

**å…³é”®ç±»**ï¼š`AlphaDataPreprocessor`

**ä¸»è¦æ–¹æ³•**ï¼š
- `prepare_training_data()`: å®Œæ•´é¢„å¤„ç†æµç¨‹
- `_clean_data()`: æ•°æ®æ¸…æ´—
- `_extract_features()`: ç‰¹å¾æå–
- `_split_dataset()`: æ•°æ®åˆ†å‰²

**è¾“å‡º**ï¼š
- `data/processed/dataset.pkl`: è®­ç»ƒ/éªŒè¯/æµ‹è¯•é›†
- `data/processed/tokenizer.pkl`: åˆ†è¯å™¨
- `data/processed/scaler.pkl`: ç‰¹å¾æ ‡å‡†åŒ–å™¨

---

### Model Layer (`models/`)

#### `tokenizer.py`
**åŠŸèƒ½**ï¼šAlphaè¡¨è¾¾å¼åˆ†è¯å’Œç¼–ç 

**å…³é”®ç±»**ï¼š`AlphaTokenizer`

**ä¸»è¦æ–¹æ³•**ï¼š
- `build_vocab_from_expressions()`: æ„å»ºè¯æ±‡è¡¨
- `encode()`: è¡¨è¾¾å¼ â†’ token IDs
- `decode()`: token IDs â†’ è¡¨è¾¾å¼
- `extract_features()`: æå–æ‰‹å·¥ç‰¹å¾

**è¯æ±‡è¡¨ç»“æ„**ï¼š
```python
{
    '<PAD>': 0,
    '<SOS>': 1,
    '<EOS>': 2,
    '<UNK>': 3,
    'ts_rank': 4,
    'winsorize': 5,
    # ... æ›´å¤štoken
}
```

#### `alpha_transformer.py`
**åŠŸèƒ½**ï¼šTransformeræ¨¡å‹å®šä¹‰

**å…³é”®ç±»**ï¼š
- `AlphaTransformerModel`: ä¸»æ¨¡å‹
- `PositionalEncoding`: ä½ç½®ç¼–ç 
- `AlphaRankingLoss`: æŸå¤±å‡½æ•°

**æ¨¡å‹æ¶æ„**ï¼š
- Token Embedding + Positional Encoding
- Transformer Encoder (multi-head attention)
- Feature Fusion (token rep + hand-crafted features)
- MLP Output Layer

#### `trainer.py`
**åŠŸèƒ½**ï¼šæ¨¡å‹è®­ç»ƒé€»è¾‘

**å…³é”®ç±»**ï¼š
- `AlphaTransformerTrainer`: è®­ç»ƒå™¨
- `AlphaDataset`: PyTorchæ•°æ®é›†

**è®­ç»ƒæµç¨‹**ï¼š
1. Setup optimizer (AdamW)
2. Training loop (forward + backward)
3. Validation (metrics computation)
4. Save best model

**è¾“å‡º**ï¼š
- `checkpoints/best_model.pt`: æœ€ä½³æ¨¡å‹
- `checkpoints/checkpoint_epoch_*.pt`: å®šæœŸæ£€æŸ¥ç‚¹

---

### Factory Layer (`factories/`)

#### `smart_factory.py`
**åŠŸèƒ½**ï¼šAIå¢å¼ºçš„æ™ºèƒ½Alphaå·¥å‚

**å…³é”®ç±»**ï¼š`SmartAlphaFactory`

**æ ¸å¿ƒåˆ›æ–°**ï¼š
```python
ä¼ ç»Ÿå·¥å‚: ç”ŸæˆNä¸ª â†’ éšæœºå›æµ‹ â†’ ç­›é€‰
æ™ºèƒ½å·¥å‚: ç”ŸæˆNä¸ª â†’ AIæ’åº â†’ åªå›æµ‹Top-K
```

**ä¸»è¦æ–¹æ³•**ï¼š
- `load_model()`: åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹
- `predict_alpha_score()`: é¢„æµ‹è¡¨è¾¾å¼åˆ†æ•°
- `generate_and_rank_alphas()`: ç”Ÿæˆ+æ’åº
- `smart_backtest_workflow()`: æ™ºèƒ½å›æµ‹å·¥ä½œæµ

**æ•ˆç‡æå‡**ï¼š
- å‡å°‘70%+æ— æ•ˆå›æµ‹
- é«˜è´¨é‡Alphaå‘ç°ç‡æå‡5-10å€

---

### UI Layer (`ui/`)

#### `app.py`
**åŠŸèƒ½**ï¼šGradioäº¤äº’å¼Webç•Œé¢

**å…³é”®ç±»**ï¼š`AlphaTransformerUI`

**é¡µé¢ç»“æ„**ï¼š
1. æ•°æ®é‡‡é›†
2. æ•°æ®é¢„å¤„ç†
3. æ¨¡å‹è®­ç»ƒï¼ˆå¸¦è®­ç»ƒæ›²çº¿ï¼‰
4. Alphaç”Ÿæˆï¼ˆå¸¦Top-Kåˆ—è¡¨ï¼‰
5. ä½¿ç”¨è¯´æ˜

**å¯åŠ¨æ–¹å¼**ï¼š
```bash
python main.py ui
# è®¿é—® http://127.0.0.1:7860
```

---

### Utils Layer (`utils/`)

#### `wq_client.py`
**åŠŸèƒ½**ï¼šWorldQuant Brain APIå®¢æˆ·ç«¯å°è£…

**å…³é”®ç±»**ï¼š`WorldQuantClient`

**ä¸»è¦æ–¹æ³•**ï¼š
- `login()`: ç™»å½•è®¤è¯
- `get_available_datafields()`: è·å–æ•°æ®å­—æ®µ
- `generate_first_order_alphas()`: ç”Ÿæˆä¸€é˜¶Alpha
- `submit_simulations()`: æäº¤å›æµ‹
- `fetch_alphas_by_performance()`: æŒ‰æ€§èƒ½ç­›é€‰Alpha

**å°è£…ä¼˜åŠ¿**ï¼š
- ç»Ÿä¸€APIæ¥å£
- è‡ªåŠ¨é‡è¯•å’Œé”™è¯¯å¤„ç†
- ç®€åŒ–è°ƒç”¨é€»è¾‘

---

## ğŸ”„ æ•°æ®æµå›¾

```
ç”¨æˆ·è¾“å…¥
    â†“
[1] collector.py â†’ data/raw/alphas_*.csv
    â†“
[2] preprocessor.py â†’ data/processed/*.pkl
    â†“
[3] trainer.py â†’ checkpoints/best_model.pt
    â†“
[4] smart_factory.py
    â”œâ”€ åŠ è½½æ¨¡å‹
    â”œâ”€ ç”Ÿæˆå€™é€‰Alpha
    â”œâ”€ AIæ’åº
    â””â”€ è¿”å›Top-K
    â†“
[5] wq_client.py â†’ æäº¤å›æµ‹
    â†“
WorldQuant Brain
```

---

## ğŸ“Œ æ–‡ä»¶ä¾èµ–å…³ç³»

```
main.py
â”œâ”€ ui/app.py
â”‚  â”œâ”€ data/collector.py
â”‚  â”œâ”€ data/preprocessor.py
â”‚  â”‚  â””â”€ models/tokenizer.py
â”‚  â”œâ”€ models/trainer.py
â”‚  â”‚  â””â”€ models/alpha_transformer.py
â”‚  â””â”€ factories/smart_factory.py
â”‚     â”œâ”€ models/alpha_transformer.py
â”‚     â”œâ”€ models/tokenizer.py
â”‚     â””â”€ utils/wq_client.py
â”‚        â””â”€ ../machine_lib.py (åŸæœ‰åº“)
â””â”€ config.py (è¢«æ‰€æœ‰æ¨¡å—å¯¼å…¥)
```

---

## ğŸ› ï¸ å¼€å‘å»ºè®®

### æ·»åŠ æ–°åŠŸèƒ½

1. **æ–°æ•°æ®æº**
   - ä¿®æ”¹ `data/collector.py`
   - æ·»åŠ æ–°çš„APIè°ƒç”¨æ–¹æ³•

2. **æ–°ç‰¹å¾**
   - ä¿®æ”¹ `models/tokenizer.py` çš„ `extract_features()`
   - æ›´æ–°ç‰¹å¾ç»´åº¦é…ç½®

3. **æ–°æ¨¡å‹æ¶æ„**
   - åœ¨ `models/` ä¸‹åˆ›å»ºæ–°æ–‡ä»¶
   - ä¿æŒä¸ `AlphaTransformerModel` ç›¸åŒçš„æ¥å£

4. **æ–°UIç»„ä»¶**
   - åœ¨ `ui/app.py` ä¸­æ·»åŠ æ–°Tab
   - å®ç°å¯¹åº”çš„åç«¯æ–¹æ³•

### è°ƒè¯•æŠ€å·§

**æ‰“å°ä¸­é—´ç»“æœ**ï¼š
```python
# åœ¨preprocessor.pyä¸­
print(f"Token IDs: {encoded[:10]}")
print(f"Features: {features}")
```

**å¯è§†åŒ–è®­ç»ƒ**ï¼š
```python
# ä½¿ç”¨tensorboard
from torch.utils.tensorboard import SummaryWriter
writer = SummaryWriter('logs/')
writer.add_scalar('Loss/train', loss, epoch)
```

**æµ‹è¯•å•ä¸ªæ¨¡å—**ï¼š
```python
# æ¯ä¸ªæ¨¡å—éƒ½æœ‰ if __name__ == "__main__" æµ‹è¯•ä»£ç 
python -m data.collector
python -m models.tokenizer
```

---

## ğŸ”’ å®‰å…¨æ³¨æ„äº‹é¡¹

### æ•æ„Ÿä¿¡æ¯

**ä¸è¦æäº¤åˆ°Git**ï¼š
- WorldQuantè´¦å·å¯†ç ï¼ˆå·²åœ¨config.pyä¸­ç¡¬ç¼–ç ï¼Œå»ºè®®æ”¹ä¸ºç¯å¢ƒå˜é‡ï¼‰
- è®­ç»ƒå¥½çš„æ¨¡å‹ï¼ˆå¤ªå¤§ï¼Œåº”å•ç‹¬å­˜å‚¨ï¼‰
- é‡‡é›†çš„åŸå§‹æ•°æ®ï¼ˆå¯èƒ½åŒ…å«æ•æ„Ÿä¿¡æ¯ï¼‰

**æ¨èåšæ³•**ï¼š
```python
# config.py
import os
username = os.getenv('WQ_USERNAME', 'default@example.com')
password = os.getenv('WQ_PASSWORD', 'default_password')
```

**è®¾ç½®ç¯å¢ƒå˜é‡**ï¼š
```bash
# Windows
set WQ_USERNAME=your_email
set WQ_PASSWORD=your_password

# Linux/Mac
export WQ_USERNAME=your_email
export WQ_PASSWORD=your_password
```

---

## ğŸ“š æ‰©å±•é˜…è¯»

- **TransformeråŸç†**: [Attention Is All You Need](https://arxiv.org/abs/1706.03762)
- **Ranking Loss**: [Learning to Rank for Information Retrieval](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/tr-2007-40.pdf)
- **WorldQuant Brain**: [å®˜æ–¹æ–‡æ¡£](https://brain.worldquantchallenge.com/docs)
- **Gradio**: [å®˜æ–¹æ•™ç¨‹](https://gradio.app/docs)

---

**æ–‡æ¡£ç»´æŠ¤**: è¯·åœ¨ä¿®æ”¹ä»£ç æ—¶åŒæ­¥æ›´æ–°ç›¸å…³æ–‡æ¡£  
**æœ€åæ›´æ–°**: 2025-10-26
